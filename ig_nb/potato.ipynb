{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['XLA_PYTHON_CLIENT_PREALLOCATE']='false'\n",
    "\n",
    "import jax\n",
    "\n",
    "from jax import random, jit, vmap, grad\n",
    "from jax import numpy as jnp\n",
    "from jax.lax import cond\n",
    "\n",
    "import astropy\n",
    "import numpy as np\n",
    "import healpy as hp\n",
    "\n",
    "import h5py\n",
    "import astropy.units as u\n",
    "\n",
    "from astropy.cosmology import Planck15, FlatLambdaCDM, z_at_value\n",
    "import astropy.constants as constants\n",
    "from jax.scipy.special import logsumexp\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.stats import gaussian_kde\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rcParams['font.family'] = 'Times New Roman'\n",
    "matplotlib.rcParams['font.sans-serif'] = ['Bitstream Vera Sans']\n",
    "matplotlib.rcParams['text.usetex'] = False\n",
    "matplotlib.rcParams['mathtext.fontset'] = 'cm'\n",
    "matplotlib.rcParams['figure.figsize'] = (16.0, 10.0)\n",
    "matplotlib.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_context('talk')\n",
    "sns.set_style('ticks')\n",
    "sns.set_palette('colorblind')\n",
    "c=sns.color_palette('colorblind')\n",
    "\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "jax.config.update('jax_default_matmul_precision', 'highest')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def timer(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        elapsed = time.time() - start\n",
    "        print(f\"{func.__name__} took {elapsed:.4f} seconds\")\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "from jaxinterp2d import interp2d, CartesianGrid\n",
    "\n",
    "H0Planck = Planck15.H0.value\n",
    "Om0Planck = Planck15.Om0\n",
    "\n",
    "zMax = 5\n",
    "zgrid = jnp.expm1(jnp.linspace(jnp.log(1), jnp.log(zMax+1), 500))\n",
    "Om0grid = jnp.linspace(Om0Planck-0.1,Om0Planck+0.1,50)\n",
    "\n",
    "cosmo = FlatLambdaCDM(H0=H0Planck,Om0=Planck15.Om0)\n",
    "speed_of_light = constants.c.to('km/s').value\n",
    "\n",
    "rs = []\n",
    "for Om0 in tqdm(Om0grid):\n",
    "    cosmo = FlatLambdaCDM(H0=H0Planck,Om0=Om0)\n",
    "    rs.append(cosmo.comoving_distance(zgrid).to(u.Mpc).value)\n",
    "\n",
    "rs = jnp.asarray(rs)\n",
    "rs = rs.reshape(len(Om0grid),len(zgrid))\n",
    "\n",
    "@jit\n",
    "def E(z,Om0=Om0Planck):\n",
    "    return jnp.sqrt(Om0*(1+z)**3 + (1.0-Om0))\n",
    "\n",
    "@jit\n",
    "def r_of_z(z,H0,Om0=Om0Planck):\n",
    "    return interp2d(Om0,z,Om0grid,zgrid,rs)*(H0Planck/H0)\n",
    "\n",
    "@jit\n",
    "def dL_of_z(z,H0,Om0=Om0Planck):\n",
    "    return (1+z)*r_of_z(z,H0,Om0)\n",
    "\n",
    "@jit\n",
    "def z_of_dL(dL,H0,Om0=Om0Planck):\n",
    "    return jnp.interp(dL,dL_of_z(zgrid,H0,Om0),zgrid)\n",
    "\n",
    "@jit\n",
    "def dV_of_z(z,H0,Om0=Om0Planck):\n",
    "    return speed_of_light*r_of_z(z,H0,Om0)**2/(H0*E(z,Om0))\n",
    "\n",
    "@jit\n",
    "def ddL_of_z(z,dL,H0,Om0=Om0Planck):\n",
    "    return dL/(1+z) + speed_of_light*(1+z)/(H0*E(z,Om0))\n",
    "\n",
    "\n",
    "GWTC1=True\n",
    "\n",
    "with h5py.File('./GWTC-3_posterior_samples_m1detm2detdLradec_4096_1peryear.h5', 'r') as inp:\n",
    "    if GWTC1:\n",
    "        nGWTC1 = 10\n",
    "        nsamps = inp.attrs['nsamp']\n",
    "        # print(nsamps)\n",
    "        nEvents = inp.attrs['nobs']\n",
    "        ra = jnp.array(inp['ra'])[:int(nGWTC1*nsamps)]\n",
    "        dec = jnp.array(inp['dec'])[:int(nGWTC1*nsamps)]\n",
    "        # m1det = jnp.array(inp['m1det'])[:int(nGWTC1*nsamps)]\n",
    "        # m2det = jnp.array(inp['m2det'])[:int(nGWTC1*nsamps)]\n",
    "        # dL = jnp.array((jnp.array(inp['dL'])*u.Mpc).value)[:int(nGWTC1*nsamps)]\n",
    "        # ra = jnp.array(inp['ra'])\n",
    "        # dec = jnp.array(inp['dec'])\n",
    "        # m1det = jnp.array(inp['m1det'])\n",
    "        # m2det = jnp.array(inp['m2det'])\n",
    "        # dL = jnp.array((jnp.array(inp['dL'])*u.Mpc).value)   \n",
    "    else:\n",
    "        nGWTC1 = 10\n",
    "        nsamps = inp.attrs['nsamp']\n",
    "        nEvents = inp.attrs['nobs'] - nGWTC1\n",
    "        ra = jnp.array(inp['ra'])[int(nGWTC1*nsamps):]\n",
    "        dec = jnp.array(inp['dec'])[int(nGWTC1*nsamps):]\n",
    "        m1det = jnp.array(inp['m1det'])[int(nGWTC1*nsamps):]\n",
    "        m2det = jnp.array(inp['m2det'])[int(nGWTC1*nsamps):]\n",
    "        dL = jnp.array((jnp.array(inp['dL'])*u.Mpc).value)[int(nGWTC1*nsamps):]\n",
    "\n",
    "print(ra.shape)\n",
    "\n",
    "nEvents=1\n",
    "\n",
    "ra = ra[:4096*nEvents]\n",
    "dec = dec[:4096*nEvents]\n",
    "\n",
    "parameter_translator = dict(\n",
    "    mass_1_det=\"m1_detector_frame_Msun\",\n",
    "    mass_2_det=\"m2_detector_frame_Msun\",\n",
    "    luminosity_distance=\"luminosity_distance_Mpc\",\n",
    "    a_1=\"spin1\",\n",
    "    a_2=\"spin2\",\n",
    "    cos_tilt_1=\"costilt1\",\n",
    "    cos_tilt_2=\"costilt2\",\n",
    ")\n",
    "\n",
    "posteriors = list()\n",
    "priors = list()\n",
    "\n",
    "file_str = \"../GWTC-1_sample_release/GW{}_GWTC-1.hdf5\"\n",
    "\n",
    "events = [\n",
    "    \"150914\",\n",
    "    \"151012\",\n",
    "    \"151226\",\n",
    "    \"170104\",\n",
    "    \"170608\",\n",
    "    \"170729\",\n",
    "    \"170809\",\n",
    "    \"170814\",\n",
    "    \"170818\",\n",
    "    \"170823\",\n",
    "]\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "m1det, m2det, dL = jnp.array([]), jnp.array([]), jnp.array([])\n",
    "# ra, dec = jnp.array([]), jnp.array([])\n",
    "\n",
    "for event in events:\n",
    "    _posterior = pd.DataFrame()\n",
    "    _prior = pd.DataFrame()\n",
    "    with h5py.File(file_str.format(event)) as ff:\n",
    "        # for my_key, gwtc_key in parameter_translator.items():\n",
    "        #     _posterior[my_key] = ff[\"IMRPhenomPv2_posterior\"][gwtc_key][:nsamps]\n",
    "        #     _prior[my_key] = ff[\"prior\"][gwtc_key][:nsamps]\n",
    "        m1det = jnp.append(m1det, ff['IMRPhenomPv2_posterior']['m1_detector_frame_Msun'][:nsamps])\n",
    "        m2det = jnp.append(m2det, ff['IMRPhenomPv2_posterior']['m2_detector_frame_Msun'][:nsamps])\n",
    "        dL = jnp.append(dL, ff['IMRPhenomPv2_posterior']['luminosity_distance_Mpc'][:nsamps])\n",
    "    posteriors.append(_posterior)\n",
    "    priors.append(_prior)\n",
    "\n",
    "GWTC3_events = {}\n",
    "posteriors = list()\n",
    "priors = list()\n",
    "\n",
    "m1det = m1det[:4096*nEvents]\n",
    "m2det = m2det[:4096*nEvents]\n",
    "dL = dL[:4096*nEvents]\n",
    "\n",
    "with open('../GWTC-3/events_names.txt', 'r') as f:                                                                                                                                                                                                                                                       \n",
    "    for line in f:\n",
    "        elements = line.strip('\\n').split()\n",
    "        GWTC3_events[elements[0]] = elements[1]\n",
    "\n",
    "parameter_translator_1 = dict(\n",
    "    # mass_1=\"mass_1_source\",\n",
    "    # mass_2=\"mass_2_source\",\n",
    "    m1det = 'mass_1',\n",
    "    m2det = 'mass_2',\n",
    "    # mass_ratio=\"mass_ratio\",\n",
    "    dL=\"luminosity_distance\",\n",
    "    # redshift=\"redshift\",\n",
    "    ra = 'ra',\n",
    "    dec = 'dec'\n",
    ")\n",
    "\n",
    "print(ra.shape)\n",
    "\n",
    "e=0 # +10 for things\n",
    "\n",
    "\n",
    "## Load samples from events\n",
    "# for event in list(GWTC3_events.keys()):\n",
    "#     # if e==60:\n",
    "#     #     break\n",
    "#     _posterior = pd.DataFrame()\n",
    "#     waveform = GWTC3_events[event]\n",
    "#     # if e>=50:\n",
    "#     # if((ra.shape[0]/4096)!=(e+10)):\n",
    "#     #     print(e, ra.shape)\n",
    "#     with h5py.File(\"../GWTC-3/{}.h5\".format(event)) as ff:\n",
    "#         # for my_key, gwtc_key in parameter_translator_1.items():\n",
    "#             # _posterior[my_key] = ff[waveform]['posterior_samples'][gwtc_key][:nsamps]\n",
    "#         m1det = jnp.append(m1det, ff[waveform]['posterior_samples']['mass_1'][:nsamps]) \n",
    "#         m2det = jnp.append(m2det, ff[waveform]['posterior_samples']['mass_2'][:nsamps])\n",
    "#         dL = jnp.append(dL, ff[waveform]['posterior_samples']['luminosity_distance'][:nsamps])\n",
    "#         ra = jnp.append(ra, ff[waveform]['posterior_samples']['ra'][:nsamps])\n",
    "#         dec = jnp.append(dec, ff[waveform]['posterior_samples']['dec'][:nsamps])\n",
    "#     posteriors.append(_posterior)\n",
    "#     e+=1\n",
    "# print(e)\n",
    "\n",
    "print(ra.shape)\n",
    "\n",
    "# nEvents = 69 \n",
    "nsamp = 4096\n",
    "ra = ra.reshape(nEvents,nsamps)[:,0:nsamp]#.flatten()\n",
    "dec = dec.reshape(nEvents,nsamps)[:,0:nsamp]#.flatten()\n",
    "m1det = m1det.reshape(nEvents,nsamps)[:,0:nsamp]#.flatten()\n",
    "m2det = m2det.reshape(nEvents,nsamps)[:,0:nsamp]#.flatten()\n",
    "dL = dL.reshape(nEvents,nsamps)[:,0:nsamp]#.flatten()\n",
    "\n",
    "ra = ra[0:nEvents].flatten()\n",
    "dec = dec[0:nEvents].flatten()\n",
    "m1det = m1det[0:nEvents].flatten()\n",
    "m2det = m2det[0:nEvents].flatten()\n",
    "dL = dL[0:nEvents].flatten()\n",
    "q = m2det/m1det\n",
    "# print(nEvents,nsamp)\n",
    "# print(len(posteriors))\n",
    "\n",
    "# jnp.savetxt('ra.txt', ra)\n",
    "# jnp.savetxt('dec.txt', dec)\n",
    "# jnp.savetxt('m1det.txt', m1det)\n",
    "# jnp.savetxt('m2det.txt', m2det)\n",
    "# jnp.savetxt('dL.txt', dL)\n",
    "# exit()\n",
    "\n",
    "# m1det1 = []\n",
    "# m2det1 = []\n",
    "# dL1 = []\n",
    "# nsamp = 4096\n",
    "\n",
    "# from scipy.stats import gaussian_kde\n",
    "# import pickle\n",
    "\n",
    "# # for i in range(nEvents):\n",
    "# #     print(f'kde_det_pkl/{i}de.pkl')\n",
    "# #     file1 = open(f'kde_det_pkl/{i}de.pkl', 'rb')\n",
    "# #     kernel = pickle.load(file1)\n",
    "# #     kde_samples = kernel.resample(size=50000).T\n",
    "    \n",
    "# #     m1det1.append(posterior[:,0][:nsamp])\n",
    "# #     m2det1.append(posterior[:,1][:nsamp])\n",
    "# #     dL1.append(posterior[:,2][:nsamp])\n",
    "\n",
    "# m1det1 = np.loadtxt('../models/m1_tkde.txt')\n",
    "# m2det1 = np.loadtxt('../models/m2_tkde.txt')\n",
    "# dL1 = np.loadtxt('../models/dL_tkde.txt')\n",
    "\n",
    "# m1det1 = m1det1.reshape(nEvents,nsamps)[:,0:nsamp]#.flatten()\n",
    "# m2det1 = m2det1.reshape(nEvents,nsamps)[:,0:nsamp]#.flatten()\n",
    "# dL1 = dL1.reshape(nEvents,nsamps)[:,0:nsamp]#.flatten()\n",
    "\n",
    "# m1det1 = m1det1[0:nEvents].flatten()\n",
    "# m2det1 = m2det1[0:nEvents].flatten()\n",
    "# dL1 = dL1[0:nEvents].flatten()\n",
    "# q = m2det1/m1det1\n",
    "\n",
    "# f = open('../potato_det.pkl', 'rb')\n",
    "# posteriors = pickle.load(f)\n",
    "\n",
    "\n",
    "# m1det1 = []\n",
    "# m2det1 = []\n",
    "# dL1 = []\n",
    "\n",
    "\n",
    "# i = 0\n",
    "# for posterior in posteriors:\n",
    "#   # print(len(posterior[:,0][:1000]))\n",
    "#   # m1det1.append(posterior['mass_1_det'][0:1000])\n",
    "#   # m2det1.append(posterior['mass_2_det'][0:1000])\n",
    "#   # dL1.append(posterior['luminosity_distance'][0:1000])\n",
    "#   if i==70:\n",
    "#     break\n",
    "\n",
    "#   if i>=60:\n",
    "#     m1det1.append(posterior[:,0][:nsamp])\n",
    "#     m2det1.append(posterior[:,1][:nsamp])\n",
    "#     dL1.append(posterior[:,2][:nsamp])\n",
    "\n",
    "#   i+=1\n",
    "\n",
    "\n",
    "# m1det1 = np.concatenate(m1det1)\n",
    "# m2det1 = np.concatenate(m2det1)\n",
    "# dL1 = np.concatenate(dL1)\n",
    "\n",
    "# m1det1[m1det1>100] = 0\n",
    "# m1det1[m1det1<0] = 0\n",
    "\n",
    "# m2det1[m2det1>100] = 0\n",
    "# m2det1[m2det1<0] = 0\n",
    "\n",
    "# nsamp = 4096\n",
    "# z1 = z_of_dL(dL1, H0Planck, Om0Planck)\n",
    "\n",
    "# Read in attributes from injection summary file\n",
    "injection_file = \"./endo3_bbhpop-LIGO-T2100113-v12.hdf5\"\n",
    "with h5py.File(injection_file, 'r') as f:\n",
    "    Tobs = f.attrs['analysis_time_s']/(365.25*24*3600) # years\n",
    "    Ndraw = f.attrs['total_generated']\n",
    "\n",
    "    m1detsels = f['injections/mass1'][:]\n",
    "    m2detsels = f['injections/mass2'][:]\n",
    "    dLsels = f['injections/distance'][:]\n",
    "    rasels = f['injections/right_ascension'][:]\n",
    "    decsels = f['injections/declination'][:]\n",
    "\n",
    "    p_draw = f['injections/sampling_pdf'][:]\n",
    "\n",
    "    pastro_cwb = f['injections/pastro_cwb'][:]\n",
    "    pastro_gstlal = f['injections/pastro_gstlal'][:]\n",
    "    pastro_mbta = f['injections/pastro_mbta'][:]\n",
    "    pastro_pycbc_bbh = f['injections/pastro_pycbc_bbh'][:]\n",
    "    pastro_pycbc_broad = f['injections/pastro_pycbc_hyperbank'][:]\n",
    "\n",
    "    ifar_cwb = f['injections/ifar_cwb'][:]\n",
    "    ifar_gstlal = f['injections/ifar_gstlal'][:]\n",
    "    ifar_mbta = f['injections/ifar_mbta'][:]\n",
    "    ifar_pycbc_bbh = f['injections/ifar_pycbc_bbh'][:]\n",
    "    ifar_pycbc_broad = f['injections/ifar_pycbc_hyperbank'][:]\n",
    "\n",
    "selection = {\n",
    "    'cwb': pastro_cwb > 0.5,\n",
    "    'gstlal': pastro_gstlal > 0.5,\n",
    "    'mbta': pastro_mbta > 0.5,\n",
    "    'pycbc_bbh': pastro_pycbc_bbh > 0.5,\n",
    "    'pycbc_broad': pastro_pycbc_broad > 0.5,\n",
    "    'any': ((pastro_cwb > 0.5) | (pastro_gstlal > 0.5) | (pastro_mbta > 0.5) |\n",
    "            (pastro_pycbc_bbh > 0.5) | (pastro_pycbc_broad > 0.5) ),\n",
    "}\n",
    "\n",
    "selection_ifar = {\n",
    "    'cwb': ifar_cwb > 1,\n",
    "    'gstlal': ifar_gstlal > 1,\n",
    "    'mbta': ifar_mbta > 1,\n",
    "    'pycbc_bbh': ifar_pycbc_bbh > 1,\n",
    "    'pycbc_broad': ifar_pycbc_broad > 1,\n",
    "    'any': ((ifar_cwb > 1) | (ifar_gstlal > 1) | (ifar_mbta > 1) |\n",
    "            (ifar_pycbc_bbh > 1) | (ifar_pycbc_broad > 1) ),\n",
    "    'cbc': ((ifar_gstlal > 1) | (ifar_pycbc_bbh > 1) | (ifar_pycbc_broad > 1) ),\n",
    "}\n",
    "\n",
    "sels = selection_ifar['cbc']\n",
    "m1detsels = jnp.array(m1detsels[sels])\n",
    "m2detsels = jnp.array(m2detsels[sels])\n",
    "dLsels = jnp.array(dLsels[sels])\n",
    "rasels = jnp.array(rasels[sels])\n",
    "decsels = jnp.array(decsels[sels])\n",
    "p_draw = jnp.array(p_draw[sels])\n",
    "\n",
    "print(Ndraw)\n",
    "print(p_draw.shape)\n",
    "print(z_of_dL(dLsels,115,Om0grid[-1]).max())\n",
    "\n",
    "# sns.distplot(z_of_dL(dLsels,H0Planck,Om0grid[-1]))\n",
    "# sns.distplot(z_of_dL(dL,H0Planck,Om0grid[-1]))\n",
    "\n",
    "m1s = (m1detsels/(1+z_of_dL(dLsels,70)))\n",
    "print(m1s.max())\n",
    "sns.distplot(m1s)\n",
    "\n",
    "\n",
    "@jit\n",
    "def dV_of_z_normed(z,Om0,gamma):\n",
    "    dV = dV_of_z(zgrid,H0Planck,Om0)*(1+zgrid)**(gamma-1)\n",
    "    prob = dV/jnp.trapezoid(dV,zgrid)\n",
    "    return jnp.interp(z,zgrid,prob)\n",
    "\n",
    "\n",
    "from jax.scipy.stats import norm\n",
    "\n",
    "mass = jnp.linspace(1, 250, 2000)\n",
    "mass_ratio =  jnp.linspace(0, 1, 2000)\n",
    "\n",
    "def Sfilter_low(m,m_min,dm_min):\n",
    "    \"\"\"\n",
    "    Smoothed filter function\n",
    "\n",
    "    See Eq. B5 in https://arxiv.org/pdf/2111.03634.pdf\n",
    "    \"\"\"\n",
    "    def f(mm,deltaMM):\n",
    "        return jnp.exp(deltaMM/mm + deltaMM/(mm-deltaMM))\n",
    "\n",
    "    S_filter = 1./(f(m-m_min,dm_min) + 1.)\n",
    "    S_filter = jnp.where(m<m_min+dm_min,S_filter,1.)\n",
    "    S_filter = jnp.where(m>m_min,S_filter,0.)\n",
    "    return S_filter\n",
    "\n",
    "def Sfilter_high(m,m_max,dm_max):\n",
    "    \"\"\"\n",
    "    Smoothed filter function\n",
    "\n",
    "    See Eq. B5 in https://arxiv.org/pdf/2111.03634.pdf\n",
    "    \"\"\"\n",
    "    def f(mm,deltaMM):\n",
    "        return jnp.exp(deltaMM/mm + deltaMM/(mm-deltaMM))\n",
    "\n",
    "    S_filter = 1./(f(m-m_max,-dm_max) + 1.)\n",
    "    S_filter = jnp.where(m>m_max-dm_max,S_filter,1.)\n",
    "    S_filter = jnp.where(m<m_max,S_filter,0.)\n",
    "    return S_filter\n",
    "\n",
    "@jit\n",
    "def logpm1_powerlaw(m1,m_min,m_max,alpha,dm_min,dm_max):\n",
    "\n",
    "    pm1 = Sfilter_low(mass,m_min,dm_min)*mass**(-alpha)*Sfilter_high(mass,m_max,dm_max)\n",
    "    pm1 = pm1/jnp.trapezoid(pm1,mass)\n",
    "    return jnp.log(jnp.interp(m1,mass,pm1))\n",
    "\n",
    "@jit\n",
    "def logpm1_peak(m1,mu,sigma):\n",
    "    pm1 =  jnp.exp(-(mass - mu)**2 / (2 * sigma ** 2))\n",
    "    pm1 = pm1/jnp.trapezoid(pm1,mass)\n",
    "    return jnp.log(jnp.interp(m1,mass,pm1))\n",
    "\n",
    "@jit\n",
    "def logpm1_powerlaw_powerlaw(m1,z,m_min_1,m_max_1,alpha_1,dm_min_1,dm_max_1,mu,sigma,f1):\n",
    "    p1 = jnp.exp(logpm1_powerlaw(m1,m_min_1,m_max_1,alpha_1,dm_min_1,dm_max_1))\n",
    "    p2 = jnp.exp(logpm1_peak(m1,mu,sigma))\n",
    "\n",
    "    pm1 = (1-f1)*p1 + f1*p2\n",
    "    return jnp.log(pm1)\n",
    "\n",
    "@jit\n",
    "def logpm1_powerlaw_GP(m1,z,mu,sigma):\n",
    "  pass\n",
    "\n",
    "@jit\n",
    "def logfq(m1,m2,beta):\n",
    "    q = m2/m1\n",
    "    pq = mass_ratio**beta\n",
    "    pq = pq/jnp.trapezoid(pq,mass_ratio)\n",
    "\n",
    "    log_pq = jnp.log(jnp.interp(q,mass_ratio,pq))\n",
    "\n",
    "    return log_pq\n",
    "\n",
    "\n",
    "@jit\n",
    "def fq(q,beta):\n",
    "    # q = m2/m1\n",
    "    pq = mass_ratio**beta\n",
    "    pq = pq/jnp.trapezoid(pq,mass_ratio)\n",
    "\n",
    "    log_pq = jnp.interp(q,mass_ratio,pq)\n",
    "\n",
    "    return log_pq\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@jit\n",
    "def log_p_pop_pl_pl(m1,m2,z,m_min_1,m_max_1,alpha_1,dm_min_1,dm_max_1,beta,mu,sigma,f1,gamma1):\n",
    "    # start_time = time.time()\n",
    "    log_dNdm1 = logpm1_powerlaw_powerlaw(m1,z,m_min_1,m_max_1,alpha_1,dm_min_1,dm_max_1,mu,sigma,f1)\n",
    "    log_dNdm2 = logpm1_powerlaw_powerlaw(m2,z,m_min_1,m_max_1,alpha_1,dm_min_1,dm_max_1,mu,sigma,f1)\n",
    "    log_fq = logfq(m1,m2,beta)\n",
    "    log_dvdz = jnp.log(dV_of_z_normed(z,Om0Planck,gamma1))\n",
    "\n",
    "    log_p_sz = np.log(0.25) # 1/2 for each spin dimension\n",
    "\n",
    "    end_time = time.time()\n",
    "    # print('time0', end_time-start_time)\n",
    "    return log_p_sz + log_dNdm1 + log_dNdm2 + log_fq + log_dvdz\n",
    "@jit\n",
    "def logdiffexp(x, y):\n",
    "    return x + jnp.log1p(jnp.exp(y-x))\n",
    "\n",
    "@jit\n",
    "def pm1_powerlaw_powerlaw(m1,m_min_1=5,m_max_1=80,alpha_1=3.3,dm_min_1=1,dm_max_1=10,mu=50,sigma=3,f1=0.4):\n",
    "    p1 = jnp.exp(logpm1_powerlaw(m1,m_min_1,m_max_1,alpha_1,dm_min_1,dm_max_1))\n",
    "    p2 = jnp.exp(logpm1_peak(m1,mu,sigma))\n",
    "\n",
    "    pm1 = (1-f1)*p1 + f1*p2\n",
    "    return pm1\n",
    "\n",
    "@jit\n",
    "def powerlaw(xx, high, low, beta=2):\n",
    "    norm = jnp.where(\n",
    "        jnp.array(beta) == -1,\n",
    "        1 / jnp.log(high / low),\n",
    "        (1 + beta) / jnp.array(high ** (1 + beta) - low ** (1 + beta)),\n",
    "    )\n",
    "    prob = jnp.power(xx, beta)\n",
    "    prob *= norm\n",
    "    prob *= (xx <= high) & (xx >= low)\n",
    "    return prob\n",
    "\n",
    "@jit\n",
    "def log_p_pop_lvk(m1,m2,z,m_min_1,m_max_1,alpha_1,dm_min_1,dm_max_1,beta,mu,sigma,f1,gamma1):\n",
    "    # start_time = time.time()\n",
    "    log_dNdm1 = logpm1_powerlaw_powerlaw(m1,z,m_min_1,m_max_1,alpha_1,dm_min_1,dm_max_1,mu,sigma,f1)\n",
    "    log_dNdm2 = logpm1_powerlaw_powerlaw(m2,z,m_min_1,m_max_1,alpha_1,dm_min_1,dm_max_1,mu,sigma,f1)\n",
    "    log_fq = logfq(m1,m2,beta)\n",
    "    log_dvdz = jnp.log(dV_of_z_normed(z,Om0Planck,gamma1))\n",
    "\n",
    "    log_p_sz = np.log(0.25) # 1/2 for each spin dimension\n",
    "\n",
    "    # end_time = time.time()\n",
    "    # print('time0', end_time-start_time)\n",
    "    return log_p_sz + log_dNdm1 + log_fq + log_dvdz\n",
    "# @jit\n",
    "# def log_p_pop_lvk(m1,m2,z,m_min_1,m_max_1,alpha_1,dm_min_1,dm_max_1,beta,mu,sigma,f1,gamma1):\n",
    "#     # start_time0 = time.time()\n",
    "    \n",
    "#     log_dVdz = jnp.log(dV_of_z_normed(z,Om0Planck,gamma1))\n",
    "#     log_p_sz = jnp.log(0.25)\n",
    "   \n",
    "#     # end_time = time.time()\n",
    "#     # print('time1', end_time-start_time0)\n",
    "    \n",
    "#     @jit\n",
    "#     def log_two_component_primary_mass_ratio(\n",
    "#         m1, m2, m_min_1, m_max_1, alpha_1, dm_min_1, dm_max_1, mu, sigma, f1\n",
    "#     ):\n",
    "#         r\"\"\"\n",
    "#         Power law model for two-dimensional mass distribution, modelling primary\n",
    "#         mass and conditional mass ratio distribution.\n",
    "    \n",
    "#         .. math::\n",
    "#             p(m_1, q) = p(m1) p(q | m_1)\n",
    "    \n",
    "#         \"\"\"\n",
    "        \n",
    "#         # start_time = time.time()\n",
    "#         log_pm1 = logpm1_powerlaw_powerlaw(m1,z,m_min_1,m_max_1,alpha_1,dm_min_1,dm_max_1,mu,sigma,f1)\n",
    "#         # p_m1 = pm1_powerlaw_powerlaw(m1, m_min_1, m_max_1, alpha_1, dm_min_1, dm_max_1, mu, sigma, f1)\n",
    "#         q = m2/m1\n",
    "#         log_pq = jnp.log(powerlaw(q, beta, 1, m_min_1/m1))\n",
    "#         # end_time = time.time()\n",
    "#         # print('time2', end_time-start_time)\n",
    "\n",
    "#         prob = log_pm1 + log_pq\n",
    "#         return prob\n",
    "    \n",
    "#     # pm1q = two_component_primary_mass_ratio(m1, m2, m_min_1, m_max_1, alpha_1, dm_min_1, dm_max_1, mu, sigma, f1)\n",
    "#     log_pm1q = log_two_component_primary_mass_ratio(\n",
    "#         m1, m2, m_min_1, m_max_1, alpha_1, dm_min_1, dm_max_1, mu, sigma, f1\n",
    "#     )\n",
    "    \n",
    "#     # end_time = time.time()\n",
    "#     # print('time3', end_time-start_time0)\n",
    "#     return log_pm1q + log_dVdz + log_p_sz\n",
    "\n",
    "# from scipy.integrate import cumtrapz\n",
    "from scipy.integrate import cumulative_trapezoid as cumtrapz\n",
    "\n",
    "## draw samples from p(z) and p(m1, q)\n",
    "def z_sampling(n_samples, gamma=3.0):\n",
    "    def pz(z, gamma=3.0):\n",
    "        dV = dV_of_z(z,H0Planck,Om0)*(1+z)**(gamma-1)\n",
    "        prob = dV/jnp.trapezoid(dV,z)\n",
    "        return prob\n",
    "\n",
    "    z_vals = jnp.linspace(0, 5, 2000)\n",
    "    pdf_zvalues = pz(z_vals, gamma)\n",
    "    cdf_zvalues = cumtrapz(pdf_zvalues, z_vals, initial=0)  # Numerical CDF\n",
    "    cdf_zvalues /= cdf_zvalues[-1]  # Normalize to [0, 1]\n",
    "\n",
    "    # Interpolate the inverse CDF\n",
    "    inverse_cdfz = interp1d(cdf_zvalues, z_vals, bounds_error=False, fill_value=(z_vals[0], z_vals[-1]))\n",
    "    u = np.random.uniform(0, 1, n_samples)  # Uniform samples\n",
    "    \n",
    "    return inverse_cdfz(u)\n",
    "\n",
    "def m1_q_samples(n_samples, m_min_1=5,m_max_1=80,alpha_1=3.3,dm_min_1=1,dm_max_1=10,beta=1,mu=50,sigma=3,f1=0.4):\n",
    "    \n",
    "    def two_component_primary_mass_ratio(\n",
    "        dataset, m_min_1, m_max_1, alpha_1, dm_min_1, dm_max_1, mu, sigma, f1\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        Power law model for two-dimensional mass distribution, modelling primary\n",
    "        mass and conditional mass ratio distribution.\n",
    "    \n",
    "        .. math::\n",
    "            p(m_1, q) = p(m1) p(q | m_1)\n",
    "    \n",
    "        \"\"\"\n",
    "\n",
    "        p_m1 = pm1_powerlaw_powerlaw(dataset[\"mass_1\"], m_min_1, m_max_1, alpha_1, dm_min_1, dm_max_1, mu, sigma, f1)\n",
    "        # p_q = powerlaw(dataset[\"mass_ratio\"], beta, 1, m_min_1/dataset[\"mass_1\"])\n",
    "        \n",
    "        p_q = fq(dataset['mass_ratio'], beta)\n",
    "        prob = p_m1 * p_q\n",
    "        return prob\n",
    "\n",
    "    m1_range = np.linspace(m_min_1+0.01, m_max_1, 2000)  # Example range for primary mass\n",
    "    q_range = np.linspace(0.01, 1, 2000)  # Example range for mass ratio\n",
    "    \n",
    "    m1_grid, q_grid = np.meshgrid(m1_range, q_range)\n",
    "    dataset = {\n",
    "        \"mass_1\": m1_grid.ravel(),\n",
    "        \"mass_ratio\": q_grid.ravel(),\n",
    "    }\n",
    "    \n",
    "    p_joint = two_component_primary_mass_ratio(dataset, m_min_1, m_max_1, alpha_1, dm_min_1, dm_max_1, mu, sigma, f1).reshape(len(q_range), len(m1_range))\n",
    "    \n",
    "    # Step 2: Normalize and compute the CDF\n",
    "    p_joint /= np.sum(p_joint)  # Normalize the joint probability\n",
    "    cdf = np.cumsum(p_joint.ravel())  # Flatten and compute cumulative sum\n",
    "    cdf /= cdf[-1]  # Normalize the CDF to [0, 1]\n",
    "    \n",
    "    # Step 3: Sample from the CDF\n",
    "    uniform_samples = np.random.uniform(0, 1, n_samples)\n",
    "    sample_indices = np.searchsorted(cdf, uniform_samples)\n",
    "    sample_q_indices, sample_m1_indices = np.unravel_index(sample_indices, p_joint.shape)\n",
    "    \n",
    "    sample_m1 = m1_range[sample_m1_indices]\n",
    "    sample_q = q_range[sample_q_indices]\n",
    "\n",
    "    return sample_m1, sample_q\n",
    "\n",
    "## Import KDEs\n",
    "kdes = []\n",
    "import pickle\n",
    "for i in range(nEvents):\n",
    "    with open(f'./kde_det_pkl/{i}de_1000.pkl', 'rb') as file:\n",
    "        kde = pickle.load(file)\n",
    "    kdes.append(kde)\n",
    "    # kdes.append((jnp.array(kde.dataset.T), kde.covariance_factor()))\n",
    "\n",
    "print(len(kdes), 'len_kde')\n",
    "seed = np.random.randint(1000)\n",
    "key = jax.random.PRNGKey(1000)\n",
    "\n",
    "@timer\n",
    "def spectral_siren_log_likelihood_nosky(gamma1=3, m_min_1=5,m_max_1=80,alpha_1=3.3,dm_min_1=1,dm_max_1=10,beta=1,mu=50,sigma=3,f1=0.4):\n",
    "    zsels = z_of_dL(dLsels, H0Planck,Om0Planck)\n",
    "    m1sels = m1detsels/(1+zsels)\n",
    "    m2sels = m2detsels/(1+zsels)\n",
    "\n",
    "    log_det_weights = log_p_pop_pl_pl(m1sels,m2sels,zsels,m_min_1,m_max_1,alpha_1,dm_min_1,dm_max_1,beta,mu,sigma,f1,gamma1)\n",
    "\n",
    "    log_det_weights += - jnp.log(p_draw) - 2*jnp.log1p(zsels) - jnp.log(ddL_of_z(zsels,dLsels,H0Planck, Om0Planck))\n",
    "\n",
    "    log_mu = logsumexp(log_det_weights) - jnp.log(Ndraw)\n",
    "    log_s2 = logsumexp(2*log_det_weights) - 2.0*jnp.log(Ndraw)\n",
    "    log_sigma2 = logdiffexp(log_s2, 2.0*log_mu - jnp.log(Ndraw))\n",
    "    Neff = jnp.exp(2.0*log_mu - log_sigma2)\n",
    "\n",
    "    ll = -jnp.inf\n",
    "    ll = jnp.where((Neff <= 4 * nEvents), ll, 0)\n",
    "    ll += -nEvents*log_mu + nEvents*(3 + nEvents)/(2*Neff)\n",
    "\n",
    "    z = z_of_dL(dL, H0Planck, Om0Planck)\n",
    "    m1 = m1det/(1+z)\n",
    "    m2 = m2det/(1+z)\n",
    "\n",
    "    weights = dL**2 / np.sum(dL**2)\n",
    "    m1 /= weights\n",
    "    m2 /= weights\n",
    "    \n",
    "    # log_weights = log_p_pop_pl_pl(m1,m2,z,m_min_1,m_max_1,alpha_1,dm_min_1,dm_max_1,beta,mu,sigma,f1,gamma1)\n",
    "    log_weights = log_p_pop_lvk(m1,m2,z,m_min_1,m_max_1,alpha_1,dm_min_1,dm_max_1,beta,mu,sigma,f1,gamma1)\n",
    "    # print('mean', jnp.mean(log_weights))\n",
    "    log_weights += - jnp.log(ddL_of_z(z,dL,H0Planck,Om0Planck)) - 2*jnp.log(dL) - 2*jnp.log1p(z) - jnp.log(m1)\n",
    "\n",
    "    nsamp1 = 4096\n",
    "    log_weights = log_weights.reshape((nEvents,nsamp1))\n",
    "    ll += jnp.sum(-jnp.log(nsamp1) + jnp.nan_to_num(logsumexp(log_weights,axis=-1)))\n",
    "\n",
    "\n",
    "    # end_time = time.time()\n",
    "    # etime = end_time - start_time\n",
    "    # print('etime', etime)\n",
    "    return ll, Neff\n",
    "\n",
    "\n",
    "def spectral_siren_log_likelihood_nosky1(gamma1=3, m_min_1=5,m_max_1=80,alpha_1=3.3,dm_min_1=1,dm_max_1=10,beta=1,mu=50,sigma=3,f1=0.4):\n",
    "    # gamma1=3\n",
    "    \n",
    "    zsels = z_of_dL(dLsels, H0Planck,Om0Planck)\n",
    "    m1sels = m1detsels/(1+zsels)\n",
    "    m2sels = m2detsels/(1+zsels)\n",
    "\n",
    "    log_det_weights = log_p_pop_pl_pl(m1sels,m2sels,zsels,m_min_1,m_max_1,alpha_1,dm_min_1,dm_max_1,beta,mu,sigma,f1,gamma1)\n",
    "\n",
    "    log_det_weights += - jnp.log(p_draw) - 2*jnp.log1p(zsels) - jnp.log(ddL_of_z(zsels,dLsels,H0Planck, Om0Planck))\n",
    "\n",
    "    log_mu = logsumexp(log_det_weights) - jnp.log(Ndraw)\n",
    "    log_s2 = logsumexp(2*log_det_weights) - 2.0*jnp.log(Ndraw)\n",
    "    log_sigma2 = logdiffexp(log_s2, 2.0*log_mu - jnp.log(Ndraw))\n",
    "    Neff = jnp.exp(2.0*log_mu - log_sigma2)\n",
    "\n",
    "    ll = -jnp.inf\n",
    "    ll = jnp.where((Neff <= 4 * nEvents), ll, 0)\n",
    "    ll += -nEvents*log_mu + nEvents*(3 + nEvents)/(2*Neff)\n",
    "\n",
    "    z = z_of_dL(dL1, H0Planck, Om0Planck)\n",
    "    m1 = m1det1/(1+z)\n",
    "    m2 = m2det1/(1+z)\n",
    "\n",
    "\n",
    "    log_weights = log_p_pop_pl_pl(m1,m2,z,m_min_1,m_max_1,alpha_1,dm_min_1,dm_max_1,beta,mu,sigma,f1,gamma1)\n",
    "\n",
    "    log_weights += - jnp.log(ddL_of_z(z,dL1,H0Planck,Om0Planck)) - 2*jnp.log(dL1) - 2*jnp.log1p(z)\n",
    "    # log_weights += - jnp.log(ddL_of_z(z,dL1,H0Planck,Om0Planck))- 2*jnp.log1p(z)\n",
    "    log_weights = log_weights.reshape((nEvents,nsamp))\n",
    "    ll += jnp.sum(-jnp.log(nsamp) + jnp.nan_to_num(logsumexp(log_weights,axis=-1)))\n",
    "\n",
    "    return ll, Neff\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from functools import partial\n",
    "import pandas as pd\n",
    "\n",
    "def get_kde_info(kdes):\n",
    "    # Extract KDE parameters\n",
    "    datasets = [kde.dataset for kde in kdes]        # List of (n_dims, n_points)\n",
    "    weights = [kde.weights for kde in kdes]         # List of (n_points,)\n",
    "    covariances = [kde.covariance for kde in kdes]  # List of scalars or matrices\n",
    "\n",
    "    # Find the maximum number of points across all KDEs\n",
    "    max_n_points = max(dataset.shape[-1] for dataset in datasets)\n",
    "    n_dims = datasets[0].shape[0]  # Dimensionality of the data\n",
    "    n_kdes = len(kdes)\n",
    "\n",
    "    # Pad datasets and weights along the number of points axis\n",
    "    padded_datasets = []\n",
    "    padded_weights = []\n",
    "    dataset_masks = []\n",
    "\n",
    "    for dataset, weight in zip(datasets, weights):\n",
    "        n_points = dataset.shape[-1]\n",
    "        padding = max_n_points - n_points\n",
    "\n",
    "        # Pad along the second axis (number of points), keeping dimensions intact\n",
    "        padded_dataset = jnp.pad(dataset, ((0, 0), (0, padding)), constant_values=0)\n",
    "        padded_weight = jnp.pad(weight, (0, padding), constant_values=0)\n",
    "        dataset_mask = jnp.pad(jnp.ones(n_points), (0, padding), constant_values=0)\n",
    "\n",
    "        padded_datasets.append(padded_dataset)\n",
    "        padded_weights.append(padded_weight)\n",
    "        dataset_masks.append(dataset_mask)\n",
    "        \n",
    "    # padded_datasets = jnp.stack(padded_datasets)   # Shape (n_kdes, n_dims, max_n_points)\n",
    "    # padded_weights = jnp.stack(padded_weights)     # Shape (n_kdes, max_n_points)\n",
    "    # dataset_masks = jnp.stack(dataset_masks)       # Shape (n_kdes, max_n_points)\n",
    "    # covariances = jnp.stack(covariances)           # Shape (n_kdes,)    \n",
    "        \n",
    "    return padded_datasets, padded_weights, dataset_masks, covariances\n",
    "\n",
    "\n",
    "def kde_eval(x, dataset, weights, covariance, mask):\n",
    "    # Extract per-dimension bandwidth from the diagonal of the covariance matrix\n",
    "    bandwidth = jnp.sqrt(jnp.diag(covariance))  # shape (n_dims,)\n",
    "\n",
    "    # Reshape x to (n_dims, 1) so it broadcasts correctly with dataset (n_dims, n_points)\n",
    "    diff = (x[:, None] - dataset) / bandwidth[:, None]   # shape (n_dims, n_points)\n",
    "\n",
    "    # Evaluate the normal PDF on each dimension and take the product over dimensions\n",
    "    kernel_vals = jnp.prod(norm.pdf(diff), axis=0)  # shape (n_points,)\n",
    "\n",
    "    # Compute density using the mask to ignore padded values and normalize appropriately\n",
    "    density = jnp.sum(weights * kernel_vals * mask) / jnp.prod(bandwidth)\n",
    "    return density\n",
    "\n",
    "@timer\n",
    "def spectral_siren_log_likelihood_nosky_kde(gamma1 = 3, m_min_1=5,m_max_1=80,alpha_1=3.3,dm_min_1=1,dm_max_1=10,beta=1,mu=50,sigma=3,f1=0.4):\n",
    "    n_samples=nEvents*nsamp\n",
    "    # gamma1 = 0 \n",
    "    # start_time = time.time()\n",
    "    zsels = z_of_dL(dLsels, H0Planck,Om0Planck)\n",
    "    m1sels = m1detsels/(1+zsels)\n",
    "    m2sels = m2detsels/(1+zsels)\n",
    "\n",
    "    log_det_weights = log_p_pop_pl_pl(m1sels,m2sels,zsels,m_min_1,m_max_1,alpha_1,dm_min_1,dm_max_1,beta,mu,sigma,f1,gamma1)\n",
    "\n",
    "    log_det_weights += - jnp.log(p_draw) - 2*jnp.log1p(zsels) - jnp.log(ddL_of_z(zsels,dLsels,H0Planck, Om0Planck))\n",
    "\n",
    "    log_mu = logsumexp(log_det_weights) - jnp.log(Ndraw)\n",
    "    log_s2 = logsumexp(2*log_det_weights) - 2.0*jnp.log(Ndraw)\n",
    "    log_sigma2 = logdiffexp(log_s2, 2.0*log_mu - jnp.log(Ndraw))\n",
    "    Neff = jnp.exp(2.0*log_mu - log_sigma2)\n",
    "\n",
    "    ll = -jnp.inf\n",
    "    ll = jnp.where((Neff <= 4 * nEvents), ll, 0)\n",
    "    ll += -nEvents*log_mu + nEvents*(3 + nEvents)/(2*Neff)\n",
    "\n",
    "    # start_time = time.time()\n",
    "    \n",
    "    m1, q = m1_q_samples(n_samples, m_min_1, m_max_1, alpha_1, dm_min_1, dm_max_1, mu, sigma, f1)\n",
    "    m2 = q * m1\n",
    "    \n",
    "    z = z_sampling(n_samples, gamma1)\n",
    "    dL = dL_of_z(z, H0=H0Planck)\n",
    "    \n",
    "    m1det = m1*(1+z)\n",
    "    m2det = m2*(1+z)\n",
    "    points = jnp.vstack([m1det, m2det, dL])\n",
    "    log_weights = np.zeros(m1det.shape[0])\n",
    "\n",
    "    kde_data = []\n",
    "\n",
    "    # end_time = time.time()\n",
    "    # etime = end_time - start_time\n",
    "    \n",
    "    # print(f'time1:{etime}')\n",
    "\n",
    "    def evaluate_kdes(points):\n",
    "        return sum(jnp.log(kde(points)) for kde in kdes)\n",
    "    \n",
    "    # def kde_sum(dataset_list, weights_list, covariance_list, mask_list, points):\n",
    "    #     results = []\n",
    "    #     for dataset, weights, covariance, mask in zip(dataset_list, weights_list, covariance_list, mask_list):\n",
    "    #         kde_values = jax.vmap(kde_eval, in_axes=(1, None, None, None, None))(\n",
    "    #                     points, dataset, weights, covariance, mask\n",
    "    #             )\n",
    "    #         results.append(kde_values)\n",
    "                \n",
    "    #     return jnp.sum(jnp.stack(results), axis=0)\n",
    "\n",
    "    # def kde_memory_efficient(dataset, weights, covariance, mask, points, point_chunk_size=256):\n",
    "    #     \"\"\"\n",
    "    #     Evaluate KDE in a memory-efficient way by processing points in chunks.\n",
    "\n",
    "    #     Args:\n",
    "    #         dataset: (3, 50000) KDE dataset\n",
    "    #         weights: (50000,) KDE weights\n",
    "    #         covariance: Covariance matrix\n",
    "    #         mask: Mask array\n",
    "    #         points: (3, 4096) array of evaluation points\n",
    "    #         point_chunk_size: Number of points to process at once\n",
    "\n",
    "    #     Returns:\n",
    "    #         KDE estimate of shape (4096,)\n",
    "    #     \"\"\"\n",
    "    #     n_points = points.shape[1]\n",
    "    #     total_pdf = jnp.zeros(n_points)\n",
    "\n",
    "    #     for j in range(0, n_points, point_chunk_size):\n",
    "    #         point_chunk = points[:, j:j + point_chunk_size]\n",
    "\n",
    "    #         kde_values = jax.vmap(\n",
    "    #             kde_eval, in_axes=(1, None, None, None, None)\n",
    "    #         )(point_chunk, dataset, weights, covariance, mask)\n",
    "\n",
    "    #         total_pdf = total_pdf.at[j:j + point_chunk_size].set(kde_values)\n",
    "\n",
    "    #     return total_pdf\n",
    "\n",
    "    \n",
    "    \n",
    "    # datasets, weights, dataset_masks, covariances = get_kde_info(kdes)\n",
    "    # datasets = jnp.stack(datasets)[0]\n",
    "    # weights = jnp.stack(weights)[0]\n",
    "    # dataset_masks = jnp.stack(dataset_masks)[0]\n",
    "    # covariances = jnp.stack(covariances)[0]\n",
    "    # print(covariances)\n",
    "    # print(datasets.shape, weights.shape, dataset_masks.shape, covariances.shape)\n",
    "    # print(points.shape, 'shape')\n",
    "\n",
    "    ## Attempt_1 batching over kde parameters - failed due to memory\n",
    "    # batched_kde_eval = jax.vmap(\n",
    "    # lambda dataset, weights, covariance, mask: jax.vmap(\n",
    "    #     kde_eval, in_axes=(1, None, None, None, None)  # x_points along axis 1\n",
    "    # )(points, dataset, weights, covariance, mask),\n",
    "    # in_axes=(0, 0, 0, 0)\n",
    "    # )\n",
    "\n",
    "    # densities = batched_kde_eval(datasets, weights, covariances, dataset_masks)\n",
    "    # results = jnp.sum(densities, axis=0)\n",
    "\n",
    "    ## Attempt_2 simply compute kde using sum(), all built-in methods - fastest\n",
    "    \n",
    "    # start_time = time.time()\n",
    "    \n",
    "    results = evaluate_kdes(points)\n",
    "\n",
    "    # end_time = time.time()\n",
    "    # etime = end_time - start_time\n",
    "    \n",
    "    # print(f'time2:{etime}')\n",
    "    ## Attempt_3 slicing data to reduce memory burden - slower than 2\n",
    "    # results = kde_memory_efficient(datasets, weights, covariances, dataset_masks, points)\n",
    "\n",
    "    # for kde in kdes:\n",
    "    #     print('num', i)\n",
    "    # log_weights += sum(Parallel(n_jobs=1)(delayed(evaluate_kde)(kde, points) for kde in kdes))\n",
    "    # log_weights += parallel_kde_evaluate(kdes, points)\n",
    "    log_weights += results\n",
    "    # print('after')\n",
    "   \n",
    "    dL1 = dL\n",
    "    log_weights += jnp.log(ddL_of_z(z,dL1,H0Planck,Om0Planck)) - 2*jnp.log(dL1) + 2*jnp.log1p(z) - jnp.log(m1)\n",
    "    # log_weights += jnp.log(ddL_of_z(z,dL1,H0Planck,Om0Planck))+ 2*jnp.log1p(z) - jnp.log(m1)\n",
    "    log_weights = log_weights.reshape((nEvents,nsamp))\n",
    "    ll += jnp.sum(-jnp.log(nsamp) + jnp.nan_to_num(logsumexp(log_weights,axis=-1)))\n",
    "\n",
    "    # end_time = time.time()\n",
    "    # etime = end_time - start_time\n",
    "    # print('etime', etime)\n",
    "    return ll, Neff\n",
    "\n",
    "from scipy.stats import norm\n",
    "# def cdf(samples):\n",
    "#     sorted_samples = np.sort(samples)\n",
    "#     # The CDF value for each sample is its rank (number of samples <= that value) divided by the total number of samples\n",
    "#     cdf_values = np.arange(1, len(sorted_samples) + 1) / len(sorted_samples)\n",
    "\n",
    "#     min_val = samples.min()\n",
    "#     max_val = samples.max()\n",
    "    \n",
    "#     U = np.zeros_like(samples)\n",
    "    \n",
    "#     # Apply normalization only where theta is not min or max\n",
    "#     mask = (samples != min_val) & (samples != max_val)\n",
    "\n",
    "#     def find_cdf(sample_value):\n",
    "#         # Ensure sample_value is a numpy array for consistent processing\n",
    "#         sample_value = np.asarray(sample_value)\n",
    "        \n",
    "#         # Initialize an array to hold the CDF results\n",
    "#         cdf_result = np.zeros_like(sample_value, dtype=float)\n",
    "        \n",
    "#         for i, value in enumerate(sample_value):\n",
    "#             # Find the index where the sample value would fit in the sorted array\n",
    "#             index = np.searchsorted(sorted_samples, value)\n",
    "#             if index == 0:\n",
    "#                 cdf_result[i] = 0.0  # If the sample value is less than the smallest sample\n",
    "#             elif index >= len(cdf_values):\n",
    "#                 cdf_result[i] = 1.0  # If the sample value is greater than the largest sample\n",
    "#             else:\n",
    "#                 cdf_result[i] = cdf_values[index - 1]  # Return the corresponding CDF value\n",
    "                \n",
    "#         return cdf_result\n",
    "\n",
    "#     U[mask] = find_cdf(samples[mask])\n",
    "#     transformed_samples = np.zeros_like(samples)\n",
    "#     transformed_samples = norm.ppf(U[mask])\n",
    "#     return transformed_samples\n",
    "\n",
    "def cdf(samples):\n",
    "    sorted_samples = np.sort(samples)\n",
    "    cdf_values = np.arange(1, len(sorted_samples) + 1) / len(sorted_samples)\n",
    "    min_val = samples.min()\n",
    "    max_val = samples.max()\n",
    "    U = np.zeros_like(samples)\n",
    "\n",
    "    # Apply normalization for all values\n",
    "    def find_cdf(sample_value):\n",
    "        sample_value = np.asarray(sample_value)\n",
    "        cdf_result = np.zeros_like(sample_value, dtype=float)\n",
    "        for i, value in enumerate(sample_value):\n",
    "            index = np.searchsorted(sorted_samples, value)\n",
    "            if index == 0:\n",
    "                cdf_result[i] = 0.0\n",
    "            elif index >= len(cdf_values):\n",
    "                cdf_result[i] = 1.0\n",
    "            else:\n",
    "                cdf_result[i] = cdf_values[index - 1]\n",
    "        return cdf_result\n",
    "\n",
    "    # Calculate U for all values\n",
    "    U = find_cdf(samples)\n",
    "\n",
    "    # Transform all values using norm.ppf\n",
    "    # Add small epsilon to avoid inf values at 0 and 1\n",
    "    epsilon = 1e-10\n",
    "    U = np.clip(U, epsilon, 1 - epsilon)\n",
    "    transformed_samples = norm.ppf(U)\n",
    "    return transformed_samples\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with open('./gmm_cdf_pkl/0de.pkl', 'rb') as f:\n",
    "    gmm = pickle.load(f)\n",
    "\n",
    "def in_cdf_transform(samples, trans):\n",
    "    U = norm.cdf(trans)\n",
    "    return np.quantile(samples, U)\n",
    "\n",
    "\n",
    "def spectral_siren_log_likelihood_nosky_gmm(gamma1=3, m_min_1=5,m_max_1=80,alpha_1=3.3,dm_min_1=1,dm_max_1=10,beta=1,mu=50,sigma=3,f1=0.4):\n",
    "    start_time = time.time()\n",
    "    n_samples=nEvents*nsamp\n",
    "    \n",
    "    zsels = z_of_dL(dLsels, H0Planck,Om0Planck)\n",
    "    m1sels = m1detsels/(1+zsels)\n",
    "    m2sels = m2detsels/(1+zsels)\n",
    "\n",
    "    log_det_weights = log_p_pop_pl_pl(m1sels,m2sels,zsels,m_min_1,m_max_1,alpha_1,dm_min_1,dm_max_1,beta,mu,sigma,f1,gamma1)\n",
    "\n",
    "    log_det_weights += - jnp.log(p_draw) - 2*jnp.log1p(zsels) - jnp.log(ddL_of_z(zsels,dLsels,H0Planck, Om0Planck))\n",
    "\n",
    "    log_mu = logsumexp(log_det_weights) - jnp.log(Ndraw)\n",
    "    log_s2 = logsumexp(2*log_det_weights) - 2.0*jnp.log(Ndraw)\n",
    "    log_sigma2 = logdiffexp(log_s2, 2.0*log_mu - jnp.log(Ndraw))\n",
    "    Neff = jnp.exp(2.0*log_mu - log_sigma2)\n",
    "\n",
    "    ll = -jnp.inf\n",
    "    ll = jnp.where((Neff <= 4 * nEvents), ll, 0)\n",
    "    ll += -nEvents*log_mu + nEvents*(3 + nEvents)/(2*Neff)\n",
    "\n",
    "    m1, q = m1_q_samples(n_samples, m_min_1, m_max_1, alpha_1, dm_min_1, dm_max_1, mu, sigma, f1)\n",
    "    m2 = q * m1\n",
    "    \n",
    "    z = z_sampling(n_samples, gamma1)\n",
    "    dL = dL_of_z(z, H0=H0Planck)\n",
    "    \n",
    "    m1det = m1*(1+z)\n",
    "    m2det = m2*(1+z)\n",
    "    points = jnp.vstack([m1det, m2det, dL])\n",
    "\n",
    "    trans_m1 = cdf(m1det)\n",
    "    trans_m2 = cdf(m2det)\n",
    "    trans_dL = cdf(dL)\n",
    "\n",
    "\n",
    "    trans_param = np.column_stack((trans_m1, trans_m2, trans_dL))\n",
    "    log_weights = np.zeros(trans_m1.shape[0])\n",
    "    results = gmm.score_samples(trans_param)\n",
    "\n",
    "    log_weights += results\n",
    "    # print('after')\n",
    "   \n",
    "    dL1 = dL\n",
    "    # log_weights += jnp.log(ddL_of_z(z,dL1,H0Planck,Om0Planck)) - 2*jnp.log(dL1) + 2*jnp.log1p(z) - jnp.log(m1)\n",
    "    log_weights += jnp.log(ddL_of_z(z,dL1,H0Planck,Om0Planck))+ 2*jnp.log1p(z) - jnp.log(m1)\n",
    "    log_weights = log_weights.reshape((nEvents,nsamp))\n",
    "    ll += jnp.sum(-jnp.log(nsamp) + jnp.nan_to_num(logsumexp(log_weights,axis=-1)))\n",
    "\n",
    "    end_time = time.time()\n",
    "    etime = end_time - start_time\n",
    "    # print('etime', etime)\n",
    "    return ll, Neff\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "true_param = [2.9, 2.35, 80, 3.5, 0.39, 10, 1.1, 50, 3, 0.4]\n",
    "\n",
    "\n",
    "# In[58]:\n",
    "\n",
    "\n",
    "gamma_low = 0\n",
    "gamma_high = 10\n",
    "\n",
    "m_min_1_low = 2\n",
    "m_min_1_high = 10\n",
    "\n",
    "m_max_1_low = 50\n",
    "m_max_1_high = 100\n",
    "\n",
    "alpha_1_low = 0\n",
    "alpha_1_high = 6\n",
    "\n",
    "dm_min_1_low = 1\n",
    "dm_min_1_high = 100\n",
    "\n",
    "dm_max_1_low = 1\n",
    "dm_max_1_high = 100\n",
    "\n",
    "beta_low = 0\n",
    "beta_high = 6\n",
    "\n",
    "mu_low = 20\n",
    "mu_high = 50\n",
    "\n",
    "sigma_low = 1\n",
    "sigma_high = 10\n",
    "\n",
    "f1_low = 0\n",
    "f1_high = 1\n",
    "\n",
    "\n",
    "lower_bound = np.array([gamma_low,m_min_1_low,m_max_1_low,alpha_1_low,dm_min_1_low,dm_max_1_low,beta_low,mu_low,sigma_low,f1_low])\n",
    "upper_bound = np.array([gamma_high,m_min_1_high,m_max_1_high,alpha_1_high,dm_min_1_high,dm_max_1_high,beta_high,mu_high,sigma_high,f1_high])\n",
    "\n",
    "# lower_bound = np.array([m_min_1_low,m_max_1_low,alpha_1_low,dm_min_1_low,dm_max_1_low,beta_low,mu_low,sigma_low,f1_low])\n",
    "# upper_bound = np.array([m_min_1_high,m_max_1_high,alpha_1_high,dm_min_1_high,dm_max_1_high,beta_high,mu_high,sigma_high,f1_high])\n",
    "\n",
    "# In[57]:\n",
    "\n",
    "\n",
    "def likelihood(coord):\n",
    "    for i in range(len(coord)):\n",
    "        if (coord[i]<lower_bound[i] or coord[i]>upper_bound[i]):\n",
    "            return -np.inf\n",
    "    ll, Neff = spectral_siren_log_likelihood_nosky(*coord)\n",
    "    if np.isnan(ll):\n",
    "        return -np.inf\n",
    "    if (Neff < 4*nEvents):\n",
    "        return -np.inf\n",
    "    else:\n",
    "        return ll\n",
    "\n",
    "\n",
    "# In[52]:\n",
    "\n",
    "\n",
    "ndims = 10\n",
    "nlive = 200\n",
    "\n",
    "# ndims=9\n",
    "\n",
    "labels = ['gamma1','m_min_1','m_max_1','alpha_1','dm_m_min_1','dm_m_max_1','beta','mu','sigma','f1']\n",
    "# labels = ['m_min_1','m_max_1','alpha_1','dm_m_min_1','dm_m_max_1','beta','mu','sigma','f1']\n",
    "\n",
    "\n",
    "def prior_transform(theta):\n",
    "    gamma1_,m_min_1_,m_max_1_,alpha_1_,dm_min_1_,dm_max_1_,beta_,mu_,sigma_,f1_ = theta\n",
    "\n",
    "    gamma1 = gamma1_*(upper_bound[0]-lower_bound[0]) + lower_bound[0]\n",
    "    m_min_1 = m_min_1_*(upper_bound[1]-lower_bound[1]) + lower_bound[1]\n",
    "    m_max_1 = m_max_1_*(upper_bound[2]-lower_bound[2]) + lower_bound[2]\n",
    "    alpha_1 = alpha_1_*(upper_bound[3]-lower_bound[3]) + lower_bound[3]\n",
    "    dm_min_1 = dm_min_1_*(upper_bound[4]-lower_bound[4]) + lower_bound[4]\n",
    "    dm_max_1 = dm_max_1_*(upper_bound[5]-lower_bound[5]) + lower_bound[5]\n",
    "    beta = beta_*(upper_bound[6]-lower_bound[6]) + lower_bound[6]\n",
    "    mu = mu_*(upper_bound[7]-lower_bound[7]) + lower_bound[7]\n",
    "    sigma = sigma_*(upper_bound[8]-lower_bound[8]) + lower_bound[8]\n",
    "    f1 = f1_*(upper_bound[9]-lower_bound[9]) + lower_bound[9]\n",
    "\n",
    "    return (gamma1,m_min_1,m_max_1,alpha_1,dm_min_1,dm_max_1,beta,mu,sigma,f1)\n",
    "\n",
    "\n",
    "# def prior_transform(theta):\n",
    "#     m_min_1_,m_max_1_,alpha_1_,dm_min_1_,dm_max_1_,beta_,mu_,sigma_,f1_ = theta\n",
    "\n",
    "#     m_min_1 = m_min_1_*(upper_bound[0]-lower_bound[0]) + lower_bound[0]\n",
    "#     m_max_1 = m_max_1_*(upper_bound[1]-lower_bound[1]) + lower_bound[1]\n",
    "#     alpha_1 = alpha_1_*(upper_bound[2]-lower_bound[2]) + lower_bound[2]\n",
    "#     dm_min_1 = dm_min_1_*(upper_bound[3]-lower_bound[3]) + lower_bound[3]\n",
    "#     dm_max_1 = dm_max_1_*(upper_bound[4]-lower_bound[4]) + lower_bound[4]\n",
    "#     beta = beta_*(upper_bound[5]-lower_bound[5]) + lower_bound[5]\n",
    "#     mu = mu_*(upper_bound[6]-lower_bound[6]) + lower_bound[6]\n",
    "#     sigma = sigma_*(upper_bound[7]-lower_bound[7]) + lower_bound[7]\n",
    "#     f1 = f1_*(upper_bound[8]-lower_bound[8]) + lower_bound[8]\n",
    "\n",
    "#     return (m_min_1,m_max_1,alpha_1,dm_min_1,dm_max_1,beta,mu,sigma,f1)\n",
    "\n",
    "\n",
    "# In[63]:\n",
    "\n",
    "\n",
    "from dynesty.utils import resample_equal\n",
    "from dynesty import NestedSampler, DynamicNestedSampler\n",
    "import multiprocessing as multi\n",
    "\n",
    "# try:\n",
    "#     pool.close()\n",
    "# except Exception:\n",
    "#     pass\n",
    "\n",
    "bound = 'multi'\n",
    "sample = 'rwalk'\n",
    "nprocesses = 1\n",
    "Dynamic = False\n",
    "\n",
    "pool = multi.Pool()\n",
    "pool.size = nprocesses\n",
    "\n",
    "if Dynamic is True:\n",
    "    dsampler = DynamicNestedSampler(likelihood, prior_transform, ndims, bound=bound, sample=sample)#, pool=pool)\n",
    "    dsampler.run_nested()\n",
    "else:\n",
    "    dsampler = NestedSampler(likelihood, prior_transform, ndims, bound=bound, sample=sample)#, pool=pool)\n",
    "    dsampler.run_nested(dlogz=0.1)\n",
    "\n",
    "\n",
    "# In[64]:\n",
    "\n",
    "\n",
    "import corner\n",
    "\n",
    "dres = dsampler.results\n",
    "\n",
    "dlogZdynesty = dres.logz[-1]        # value of logZ\n",
    "dlogZerrdynesty = dres.logzerr[-1]  # estimate of the statistcal uncertainty on logZ\n",
    "\n",
    "# output marginal likelihood\n",
    "print('Marginalised evidence (using dynamic sampler) is {}  {}'.format(dlogZdynesty, dlogZerrdynesty))\n",
    "\n",
    "# get the posterior samples\n",
    "dweights = np.exp(dres['logwt'] - dres['logz'][-1])\n",
    "dpostsamples = resample_equal(dres.samples, dweights)\n",
    "\n",
    "print('Number of posterior samples (using dynamic sampler) is {}'.format(dpostsamples.shape[0]))\n",
    "\n",
    "fig = corner.corner(dpostsamples, labels=labels, hist_kwargs={'density': True})\n",
    "\n",
    "plt.show()\n",
    "plt.savefig('./iggy_gwtc3-t.png')\n",
    "\n",
    "import pickle\n",
    "\n",
    "# open a file, where you ant to store the data\n",
    "file = open('plbump-GWTC3-t.pkl', 'wb')\n",
    "\n",
    "# dump information to that file\n",
    "pickle.dump(dres, file)\n",
    "\n",
    "# close the file\n",
    "file.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax",
   "language": "python",
   "name": "jax"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
