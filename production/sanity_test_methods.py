
import argparse
import matplotlib
matplotlib.use("Agg")

def parse_args():
    parser = argparse.ArgumentParser(description="Sanity GWTC3 GMM Method1-3")
    parser.add_argument("--outdir", type=str, default="outputs", help="Output directory")
    parser.add_argument("--tag", type=str, default="sanity", help="Filename tag")
    return parser.parse_args()

args = parse_args()
import os
os.makedirs(args.outdir, exist_ok=True)

# -*- coding: utf-8 -*-
"""Copy of GWTC3_gmm_method1to3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1V_vWMwYUgkb1kMIm0CQ1JCRiNIDvwxNp
"""

# !pip install healpy
# !pip install dynesty
# !pip install corner

CB = {
    "blue":   "#0072B2",
    "orange": "#E69F00",
    "green":  "#009E73",
    "red":    "#D55E00",
    "purple": "#CC79A7",
    "gray":   "#999999",
}

# Commented out IPython magic to ensure Python compatibility.
import os
os.environ['XLA_PYTHON_CLIENT_PREALLOCATE']='false'

import jax

from jax import random, jit, vmap, grad
from jax import numpy as jnp
from jax.lax import cond

import astropy
import numpy as np
import healpy as hp

import h5py
import astropy.units as u

from astropy.cosmology import Planck15, FlatLambdaCDM, z_at_value
import astropy.constants as constants
from jax.scipy.special import logsumexp
from scipy.interpolate import interp1d
from scipy.stats import gaussian_kde
from tqdm import tqdm

import matplotlib
# %matplotlib inline

import matplotlib.pyplot as plt
matplotlib.rcParams['font.family'] = 'Times New Roman'
matplotlib.rcParams['font.sans-serif'] = ['Bitstream Vera Sans']
matplotlib.rcParams['text.usetex'] = False
matplotlib.rcParams['mathtext.fontset'] = 'cm'
matplotlib.rcParams['figure.figsize'] = (16.0, 10.0)
matplotlib.rcParams['axes.unicode_minus'] = False

import seaborn as sns
sns.set_context('talk')
sns.set_style('ticks')
sns.set_palette('colorblind')
c=sns.color_palette('colorblind')

jax.config.update("jax_enable_x64", True)
jax.config.update('jax_default_matmul_precision', 'highest')

from jaxinterp2d import interp2d, CartesianGrid

H0Planck = Planck15.H0.value
Om0Planck = Planck15.Om0
speed_of_light = constants.c.to('km/s').value

zMax = 15.0
zgrid = jnp.expm1(np.linspace(np.log(1), np.log(zMax+1), 10000))
Om0grid = jnp.linspace(0,1,1000)

rs = []
for Om0 in tqdm(Om0grid):
    cosmo = FlatLambdaCDM(H0=H0Planck,Om0=Om0)
    rs.append(cosmo.comoving_distance(zgrid).to(u.Mpc).value)

rs = jnp.asarray(rs)
rs = rs.reshape(len(Om0grid),len(zgrid))

@jit
def E(z,Om0=Om0Planck):
    return jnp.sqrt(Om0*(1+z)**3 + (1.0-Om0))

@jit
def r_of_z(z,H0,Om0=Om0Planck):
    return interp2d(Om0,z,Om0grid,zgrid,rs)*(H0Planck/H0)

@jit
def dL_of_z(z,H0,Om0=Om0Planck):
    return (1+z)*r_of_z(z,H0,Om0)

@jit
def z_of_dL(dL,H0,Om0=Om0Planck):
    return jnp.interp(dL,dL_of_z(zgrid,H0,Om0),zgrid)

@jit
def dV_of_z(z,H0,Om0=Om0Planck):
    return speed_of_light*r_of_z(z,H0,Om0)**2/(H0*E(z,Om0))

@jit
def ddL_of_z(z,dL,H0,Om0=Om0Planck):
    return dL/(1+z) + speed_of_light*(1+z)/(H0*E(z,Om0))

from scipy.stats import norm

# Define pop model in source frame
mu = 35
sigma = 1

ppop_m1 = norm(mu,sigma)

muz = 1
sigmaz = 0.1

ppop_z = norm(muz,sigmaz)

# draw N observations from pop model (IN SOURCE FRAME)

Nobs = 10

m1s = ppop_m1.rvs(Nobs)

zs = ppop_z.rvs(Nobs)

# Convert observations to detector frame

m1sdet = (1 + zs)*m1s
dLs = dL_of_z(zs,H0Planck,Om0Planck)

# plt.scatter(m1sdet,dLs)
# plt.show()

# now lets add uncertainty (draw 1000 samples for each events with sigma_m1det = 5, sigma_dL = 100)

# also compute kdes for second method
from scipy.stats import gaussian_kde

sigma_m1det = 5
sigma_dL = 100
nsamp = 1000

m1det_ = []
dL_ = []
kdes = []
for k in range(Nobs):
    m1det_samples = norm(m1sdet[k],sigma_m1det).rvs(nsamp)
    dL_samples = norm(dLs[k],sigma_dL).rvs(nsamp)
    kdes.append(gaussian_kde(np.vstack([m1det_samples,dL_samples])))
    m1det_.append(m1det_samples)
    dL_.append(dL_samples)

# # plot the first ten
# for k in range(Nobs):
#     plt.scatter(m1det_[k],dL_[k])
# plt.show()

#Inference method 1. use samples directly with pop models define via analytical parametric models above
from jax.scipy.stats import norm as norm_jax

# put data into a 1d jnp array

m1det = jnp.concatenate(m1det_)
dL = jnp.concatenate(dL_)

@jit
def likelihood_method_1(mu_m,sigma_m,mu_z,sigma_z):

    z = z_of_dL(dL, H0Planck, Om0Planck)
    m1 = m1det/(1+z)

    log_weights = norm_jax.logpdf(m1,mu_m,sigma_m) + norm_jax.logpdf(z,mu_z,sigma_z)

    log_weights += - jnp.log(ddL_of_z(z,dL,H0Planck,Om0Planck)) - jnp.log1p(z) # jacobian

    log_weights = log_weights.reshape((Nobs,nsamp))
    ll = jnp.sum(-jnp.log(nsamp) + logsumexp(log_weights,axis=-1))

    return ll


def loglike_method_1(coord):
    mu_m,sigma_m,mu_z,sigma_z = coord
    ll = likelihood_method_1(mu_m,sigma_m,mu_z,sigma_z)
    if np.isnan(ll):
        return -np.inf
    else:
        return ll

# likelihood_method_1(mu,sigma,muz,sigmaz)

#priors

mu_m_lo,sigma_m_lo,mu_z_lo,sigma_z_lo = (25, 0.1, 0.1, 0.01)
mu_m_hi,sigma_m_hi,mu_z_hi,sigma_z_hi = (45, 10,  2.0, 0.5)
lower_bound = np.array([mu_m_lo,sigma_m_lo,mu_z_lo,sigma_z_lo])
upper_bound = np.array([mu_m_hi,sigma_m_hi,mu_z_hi,sigma_z_hi])


ndims = len(lower_bound)
nlive = 1000


labels = [
    r"$\mu$",
    r"$\sigma$",
    r"$\mu_z$",
    r"$\sigma_z$",
]


def prior_transform(theta):
    transformed_params = [
        theta[i] * (upper_bound[i] - lower_bound[i]) + lower_bound[i]
        for i in range(len(theta))
    ]

    return tuple(transformed_params)

#sampling

from dynesty.utils import resample_equal
from dynesty import NestedSampler, DynamicNestedSampler
import multiprocessing as multi

bound = 'multi'
sample = 'rwalk'
nprocesses = 1

dsampler = NestedSampler(loglike_method_1, prior_transform, ndims, bound=bound, sample=sample, nlive=nlive)
dsampler.run_nested(dlogz=0.1)

import corner

dres = dsampler.results

dlogZdynesty = dres.logz[-1]        # value of logZ
dlogZerrdynesty = dres.logzerr[-1]  # estimate of the statistcal uncertainty on logZ

# output marginal likelihood
print('Marginalised evidence (using dynamic sampler) is {} ± {}'.format(dlogZdynesty, dlogZerrdynesty))

# get the posterior samples
dweights = np.exp(dres['logwt'] - dres['logz'][-1])
dpostsamples = resample_equal(dres.samples, dweights)

print('Number of posterior samples (using dynamic sampler) is {}'.format(dpostsamples.shape[0]))

fig = corner.corner(dpostsamples,  hist_kwargs={'density': True},labels=labels,truths=[mu,sigma,muz,sigmaz], color=CB['blue'])
# plt.show()

from sklearn.mixture import GaussianMixture
import numpy as np
import jax.scipy as jsp

@jax.jit
def gmm_logpdf_optimized(x_batch, weights, means, precisions, logdets):
    """
    x_batch:   (N, D)
    weights:   (K,)
    means:     (K, D)
    precisions:(K, D, D)
    logdets:   (K,)
    returns:   (N,)
    """
    D = means.shape[-1]
    diffs = x_batch[:, None, :] - means[None, :, :]                # (N,K,D)
    quad = jnp.einsum('nkd,kdj,nkj->nk', diffs, precisions, diffs)  # (N,K)
    log_comp = -0.5 * (D*jnp.log(2*jnp.pi) + logdets + quad)       # (N,K)
    logw = jnp.log(jnp.clip(weights, 1e-12, 1.0))[None, :]         # (1,K)
    return jsp.special.logsumexp(logw + log_comp, axis=-1)         # (N,)

K = 7  # try BIC/AIC later
gmms = []
for e in range(Nobs):
    X_e = np.column_stack([np.asarray(m1det_[e]), np.asarray(dL_[e])])  # (N_e, D)
    gmm = GaussianMixture(
        n_components=K, covariance_type='full',
        reg_covar=1e-6, n_init=10, random_state=0
    ).fit(X_e)
    gmms.append(gmm)

# Stack params
wts_np   = np.stack([g.weights_              for g in gmms])       # (E,K)
means_np = np.stack([g.means_                for g in gmms])       # (E,K,D)

# Precisions (and their Cholesky) are directly provided by sklearn
prec_np       = np.stack([g.precisions_             for g in gmms])        # (E,K,D,D)
prec_chol_np  = np.stack([g.precisions_cholesky_    for g in gmms])        # (E,K,D,D)

# log|Σ| = -log|Precision| = -2 * sum(log diag(L_precision))
logdets_np = -2.0 * np.sum(
    np.log(np.diagonal(prec_chol_np, axis1=-2, axis2=-1)),
    axis=-1
)  # (E,K)

# → JAX float64
wts       = jnp.asarray(wts_np,      dtype=jnp.float64)   # (E,K)
mus       = jnp.asarray(means_np,    dtype=jnp.float64)   # (E,K,D)
precisions= jnp.asarray(prec_np,     dtype=jnp.float64)   # (E,K,D,D)
logdets   = jnp.asarray(logdets_np,  dtype=jnp.float64)   # (E,K)

_event_kernel = lambda X, w, m, P, ld: gmm_logpdf_optimized(X, w, m, P, ld)
gmm_logpdf_per_event = jax.vmap(_event_kernel, in_axes=(None, 0, 0, 0, 0), out_axes=0)  # (E,N)

import jax
import jax.numpy as jnp
import jax.scipy as jsp

@jax.jit
def logpdf_sum_over_events_with_mega_eval(X, wts, mus, precisions, logdets):
    """
    X:          (N, D)           samples theta ~ pop(theta|lambda)  (same X for all events)
    wts:        (E, K)
    mus:        (E, K, D)
    precisions: (E, K, D, D)
    logdets:    (E, K)

    Returns:
      logpdf_per_event: (E, N)  with log p_e(X_n)
      total_logpdf:     ()      equals sum_e log p_e(X_n) if you later sum over e (or whatever reduction you use)
    """
    N, D = X.shape
    E, K = wts.shape

    # ---- Mega evaluation over all EK components (no cross-event averaging) ----
    EK = E * K
    M  = mus.reshape(EK, D)                 # (EK, D)
    P  = precisions.reshape(EK, D, D)       # (EK, D, D)
    LD = logdets.reshape(EK)                # (EK,)

    # Component log-densities at all X: (N,EK)
    diffs = X[:, None, :] - M[None, :, :]                       # (N,EK,D)
    quad  = jnp.einsum('nkd,kdj,nkj->nk', diffs, P, diffs)      # (N,EK)
    log_comp = -0.5 * (D*jnp.log(2*jnp.pi) + LD + quad)         # (N,EK)

    # ---- Reshape back to (N,E,K), then do per-event log-sum-exp over K ----
    log_comp_NEK = log_comp.reshape(N, E, K)                    # (N,E,K)
    logw_NEK     = jnp.log(jnp.clip(wts, 1e-12, 1.0))[None,:,:] # (1,E,K)

    logp_N_E = jsp.special.logsumexp(logw_NEK + log_comp_NEK, axis=-1)  # (N,E)
    logp_E_N = jnp.swapaxes(logp_N_E, 0, 1)                              # (E,N)

    # Example reduction that matches your loop “Logpdf += gmm_e.logpdf(thetas)”:
    # (you may keep logp_E_N to apply Jacobians/weights per event/sample first)
    total_logpdf = jnp.sum(logp_E_N)  # if your loop literally sums event logpdfs over thetas

    return logp_E_N, total_logpdf

# Now METHOD 2

nsamp_pop = 20000
master_key = jax.random.PRNGKey(0)  # Seed for reproducibility

from jax.scipy.stats import gaussian_kde as gaussian_kde_jax
import time

N = nsamp_pop
# u_base = jnp.linspace(0.5/N, 1-0.5/N, N*2).reshape(2, N)  # 3 sets of stratified uniforms
u_m1 = (jnp.arange(N) + 0.5) / N
u_z  = (u_m1  + 0.73) % 1.0

def make_eps(u):
    half = jnp.floor_divide(N, 2)
    u_half = (jnp.arange(half) + 0.5) / N
    eps = jsp.special.ndtri(u_half)
    return jnp.concatenate([eps, -eps]) if N % 2 == 0 else jsp.special.ndtri(u)

eps_m1 = make_eps(u_m1)
eps_z  = make_eps(u_z)


@jit
def likelihood_method_2(mu_m,sigma_m,mu_z,sigma_z,subkey):
    # start_time = time.time()
    m1s_pop = mu_m + sigma_m * eps_m1
    zs_pop = mu_z + sigma_z * eps_z

    dLs_pop = dL_of_z(zs_pop, H0Planck, Om0Planck)
    m1dets_pop = m1s_pop*(1+zs_pop)

    X = jnp.column_stack([m1dets_pop, dLs_pop])  # (N,2), original units

    # log p_event(X) for all events: (E,N)
    # logp_E_N = gmm_logpdf_per_event(X, wts, mus, precisions, logdets)
    logp_E_N, total = logpdf_sum_over_events_with_mega_eval(X, wts, mus, precisions, logdets)

    # Same Jacobian as your KDE version (original units)
    logJ = jnp.log(ddL_of_z(zs_pop, dLs_pop, H0Planck, Om0Planck)) + jnp.log1p(zs_pop)  # (N,)
    per_event_log_like = jnp.nan_to_num(jsp.special.logsumexp(logp_E_N + logJ[None, :], axis=1)) - jnp.log(nsamp_pop)  # (E,)

    # end_time = time.time()
    # duration = end_time - start_time

    # jax.debug.print('duration{}', duration)
    return jnp.sum(per_event_log_like)

def loglike_method_2(coord):
    mu_m,sigma_m,mu_z,sigma_z = coord
    global master_key
    master_key, subkey = jax.random.split(master_key)

    ll = likelihood_method_2(mu_m,sigma_m,mu_z,sigma_z,subkey)
    if np.isnan(ll):
        return -np.inf
    else:
        return ll

#sampling

from dynesty.utils import resample_equal
from dynesty import NestedSampler, DynamicNestedSampler
import multiprocessing as multi

bound = 'multi'
sample = 'rwalk'
nprocesses = 1

dsampler = NestedSampler(loglike_method_2, prior_transform, ndims, bound=bound, sample=sample, nlive=nlive)
dsampler.run_nested(dlogz=0.1)

import corner

dres = dsampler.results

dlogZdynesty = dres.logz[-1]        # value of logZ
dlogZerrdynesty = dres.logzerr[-1]  # estimate of the statistcal uncertainty on logZ

# output marginal likelihood
print('Marginalised evidence (using dynamic sampler) is {} ± {}'.format(dlogZdynesty, dlogZerrdynesty))

# get the posterior samples
dweights = np.exp(dres['logwt'] - dres['logz'][-1])
dpostsamples = resample_equal(dres.samples, dweights)

print('Number of posterior samples (using dynamic sampler) is {}'.format(dpostsamples.shape[0]))

fig = corner.corner(dpostsamples,  hist_kwargs={'density': True},labels=labels,truths=[mu,sigma,muz,sigmaz], fig=fig, color=CB['orange'])
# plt.show()

def build_q_mixture(event_weights, event_means, event_prec, event_logdetS):
    """Concatenate all event GMMs to form q(x) = (1/E) * sum_e p_e(x)."""
    E = len(event_weights)
    # Concatenate along component axis
    w_all   = jnp.concatenate(event_weights, axis=0)        # (K_total,)
    mu_all  = jnp.concatenate(event_means, axis=0)          # (K_total, D)
    Prec_all= jnp.concatenate(event_prec, axis=0)           # (K_total, D, D)
    ldetS_all = jnp.concatenate(event_logdetS, axis=0)      # (K_total,)
    # Scale by 1/E to make q the simple average of posteriors
    w_q = w_all / E
    # Numerically renorm to ensure sum=1 (tiny drift prevention)
    w_q = w_q / jnp.sum(w_q)
    # Precompute Cholesky of precision for fast sampling (Prec = L @ L.T with L lower-tri)
    L_prec = jnp.linalg.cholesky(Prec_all)                  # (K_total, D, D)
    return w_q, mu_all, Prec_all, L_prec, ldetS_all

def sample_from_q(key, N, w_q, mu_all, L_prec):
    """
    Vectorized sampling from the global mixture q.
    Returns:
      x: (N, D), comp_idx: (N,) chosen component indices
    """
    K_total, D = mu_all.shape
    # Sample component indices ~ Categorical(w_q)
    key, sk1, sk2 = random.split(key, 3)
    comp_idx = random.choice(sk1, K_total, shape=(N,), p=w_q, replace=True)  # (N,)

    # Gather component params
    mu_ND   = mu_all[comp_idx]           # (N, D)
    L_NDD   = L_prec[comp_idx]           # (N, D, D), lower-tri where Prec = L @ L^T

    # Standard normals
    z = random.normal(sk2, shape=(N, D)) # (N, D)

    # Solve L * y = z → y = L^{-1} z  (triangular solve per sample)
    # jnp.linalg.solve works but triangular_solve is faster/stable
    y = jax.scipy.linalg.solve_triangular(L_NDD, z[..., None], lower=True).squeeze(-1)  # (N, D)

    x = mu_ND + y
    return x, comp_idx

def _log_gauss_with_precision(x_ND, mu_KD, Prec_KDD, logdetS_K):
    # x: (N,D), mu: (K,D), Prec: (K,D,D), logdetS: (K,)
    diffs = x_ND[:, None, :] - mu_KD[None, :, :]                    # (N,K,D)
    quad  = jnp.einsum('nkd,kdj,nkj->nk', diffs, Prec_KDD, diffs)   # (N,K)
    D = x_ND.shape[1]
    return -0.5 * (D*jnp.log(2*jnp.pi) + logdetS_K[None, :] + quad) # (N,K)

def exact_log_qx_padded(x, w_q, mu_all, Prec_all, logdetS_all, chunk_K=2048):
    """
    Exact log q(x) with chunking but without dynamic slicing.
    Pads K to a multiple of chunk_K and folds with logaddexp.
    """
    N, D   = x.shape
    K      = mu_all.shape[0]
    n_chunks = (K + chunk_K - 1) // chunk_K
    K_pad  = n_chunks * chunk_K
    pad    = K_pad - K

    # Pad along component axis; padded comps get weight=0 → logw=-inf, so no contribution
    def pad1(a, val=0.0):
        return jnp.pad(a, ((0, pad),) + ((0, 0),)*(a.ndim-1), constant_values=val)

    w_pad     = pad1(w_q)
    mu_pad    = pad1(mu_all)
    Prec_pad  = pad1(Prec_all)
    ldet_pad  = pad1(logdetS_all)

    # Reshape into (n_chunks, chunk_K, ...)
    w_rs    = w_pad.reshape(n_chunks, chunk_K)
    mu_rs   = mu_pad.reshape(n_chunks, chunk_K, D)
    Prec_rs = Prec_pad.reshape(n_chunks, chunk_K, D, D)
    ldet_rs = ldet_pad.reshape(n_chunks, chunk_K)

    logw_rs = jnp.log(jnp.clip(w_rs, 1e-300, 1.0))  # (n_chunks, chunk_K)

    def body(carry, i):
        # per-chunk logsumexp over components, then accumulate over chunks
        logN = _log_gauss_with_precision(x, mu_rs[i], Prec_rs[i], ldet_rs[i])  # (N, chunk_K)
        chunk_sum = logsumexp(logw_rs[i][None, :] + logN, axis=1)              # (N,)
        return jnp.logaddexp(carry, chunk_sum), None

    init = jnp.full((N,), -jnp.inf)
    log_qx, _ = jax.lax.scan(body, init, jnp.arange(n_chunks))
    return log_qx


def sample_and_logq_from_q(key, N, q_params, approx=False, top_T=16, chunk_K=2048):
    w_q, mu_all, Prec_all, L_prec, logdetS_all = q_params
    x, comp_idx = sample_from_q(key, N, w_q, mu_all, L_prec)
    log_qx = exact_log_qx_padded(x, w_q, mu_all, Prec_all, logdetS_all, chunk_K=chunk_K)
    return x, log_qx, comp_idx

q_params = build_q_mixture(wts, mus, precisions, logdets)

x, log_qx, _ = sample_and_logq_from_q(master_key, 15000, q_params, approx=True, top_T=16)

nsamp_pop = 15000

x, log_qx, _ = sample_and_logq_from_q(master_key, nsamp_pop, q_params, approx=True, top_T=16)
m1dets_pop, dLs_pop = x[:, 0], x[:, 1]

zs_pop = z_of_dL(dLs_pop, H0Planck)
m1s_pop = m1dets_pop/(1+zs_pop)

X = jnp.column_stack([m1dets_pop, dLs_pop])
logp_E_N, total = logpdf_sum_over_events_with_mega_eval(X, wts, mus, precisions, logdets)

logJ = - jnp.log(ddL_of_z(zs_pop, dLs_pop, H0Planck, Om0Planck)) - jnp.log1p(zs_pop)  # (N,)


@jit
def likelihood_method_3(mu_m,sigma_m,mu_z,sigma_z,subkey):
    # start_time = time.time()
    log_pop = norm_jax.logpdf(m1s_pop,mu_m,sigma_m) + norm_jax.logpdf(zs_pop,mu_z,sigma_z)
    log_weights = log_pop - log_qx
    # log p_event(X) for all events: (E,N)
    # logp_E_N = gmm_logpdf_per_event(X, wts, mus, precisions, logdets)

    # Same Jacobian as your KDE version (original units)
    per_event_log_like = jnp.nan_to_num(jsp.special.logsumexp(log_weights[None, :] + logp_E_N + logJ[None, :], axis=1)) - jnp.log(nsamp_pop)  # (E,)

    # end_time = time.time()
    # duration = end_time - start_time

    # jax.debug.print('duration{}', duration)
    return jnp.sum(per_event_log_like)

def loglike_method_3(coord):
    mu_m,sigma_m,mu_z,sigma_z = coord
    global master_key
    master_key, subkey = jax.random.split(master_key)

    ll = likelihood_method_3(mu_m,sigma_m,mu_z,sigma_z,subkey)
    if np.isnan(ll):
        return -np.inf
    else:
        return ll

#sampling

from dynesty.utils import resample_equal
from dynesty import NestedSampler, DynamicNestedSampler
import multiprocessing as multi

bound = 'multi'
sample = 'rwalk'
nprocesses = 1

dsampler = NestedSampler(loglike_method_3, prior_transform, ndims, bound=bound, sample=sample, nlive=nlive)
dsampler.run_nested(dlogz=0.1)

import corner

dres = dsampler.results

dlogZdynesty = dres.logz[-1]        # value of logZ
dlogZerrdynesty = dres.logzerr[-1]  # estimate of the statistcal uncertainty on logZ

# output marginal likelihood
print('Marginalised evidence (using dynamic sampler) is {} ± {}'.format(dlogZdynesty, dlogZerrdynesty))

# get the posterior samples
dweights = np.exp(dres['logwt'] - dres['logz'][-1])
dpostsamples = resample_equal(dres.samples, dweights)

print('Number of posterior samples (using dynamic sampler) is {}'.format(dpostsamples.shape[0]))

fig = corner.corner(dpostsamples,  hist_kwargs={'density': True},labels=labels, color=CB['green'],truths=[mu,sigma,muz,sigmaz], fig=fig)
# plt.show()

handles = [
    mlines.Line2D([], [], color=CB["blue"],   lw=2, label="Method 1"),
    mlines.Line2D([], [], color=CB["orange"], lw=2, label="Method 2"),
    mlines.Line2D([], [], color=CB["green"],  lw=2, label="Method 3"),
]

fig.legend(
    handles=handles,
    loc="upper right",
    frameon=False,
    fontsize=12,
)


# ---- save final figure if any ----
import matplotlib.pyplot as plt
fig = plt.gcf()
outfile = os.path.join(args.outdir, f"corner_{args.tag}.pdf")
fig.savefig(outfile, bbox_inches="tight")
plt.close(fig)
print(f"[INFO] Saved figure to {outfile}")
